---
layout: post
title:  "python多线程"
categories: python
tags: python python-concurrence
author: 大可
---

## GIL
- Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁
- 对于IO操作来说，一般使用底层的C来实现，在调用C时，会释放GIL锁，故此时的多线程是有效的，在等待IO的过程中，其他线程可以继续执行，但是对于普通CPU计算来说，每个线程只有在获得GIL锁的情况下才能执行，故此时同一时刻只能有一个线程在执行，故效率甚至不如单线程执行
- 由于多线程的GIL锁机制，导致多线程并不能利用多核，如果要充分利用多核性能，需要使用多进程

## ThreadLocal
如果时线程间共享的信息，需要借助线程同步机制，但对于并非线程间共享的内容，可以使用ThreadLocal

### 场景
ThreadLocal最常用的地方是为每一个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。

### 示例
如下，全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。

```python
# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

def thread_local_learn():
    t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
    t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
    t1.start()
    t2.start()
    ## 将该线程添加到当前进程并等待
    t1.join()
    t2.join()
    ## 线程是否存活
    print(t1.is_alive())
```

## 同步机制

### 锁
Lock和RLock的区别在于Lock在被某线程拥有未释放时，无论是拥有者线程还是其他线程再次请求锁时都会阻塞，而RLock可被拥有者线程多次获取（acquire），同时，拥有者线程如果要释放锁，需要释放（release）与请求（require）相同的次数

#### RLock场景
假设一种情形：

```python
#coding=utf-8
import threading
import time

m_lock = threading.Lock()

def h():
    with m_lock:
        print 'h'

def g():
    with m_lock:
        print 'g'

h()
g()
```

上面的例子中，h()和g()中都用了Lock，在多线程环境下，他们可以做到相安无事，但是，程序的结构总是处于变化中，尤其是那些庞大的系统，一个小小的变化可能牵一发而动全身，假设发生了下面的变化：

```python
#coding=utf-8
import threading
import time

m_lock = threading.Lock()

def h():
    with m_lock:
        g()
        print 'h'

def g():
    with m_lock:
        print 'g'

h()
g()
```

此时程序便产生死锁，故在深层次调用过程中，使用RLock是非常安全的选择

### Event
event并没有锁，而是依赖信号标志，当信号标志为True，**唤醒所有等待的线程**

```python
def use_event(event):
    print("starting threading")
    ## 设置信号标志为True，唤醒所有等待的线程
    time.sleep(2)
    event.set()
    time.sleep(2)
    print("ending threading")

def wait_thread(event):
    event.wait()
    print("singal is True in wait thread")

def wait_event():
    event = threading.Event()
    t = threading.Thread(target=use_event, args=(event,))
    t.start()
    t2 = threading.Thread(target=wait_thread, args=(event,))
    t2.start()
    event.wait()
    print("singal is True")
```

### Condition
使用Condition,condition一次性仅可唤醒一个等待的进程，condition可看作是event+lock（默认为RLock）的组合，在针对共享资源访问的同步问题上，可以使用condition来实现

```python
def wait_condition(condition, n):
    print("start {}".format(n))
    with condition:
        print("before", n)
        ## 调用wait时会先创建新的锁（Lock），并堵塞自己，将锁存入self._waiters中(self指condition对象)，然后将condition潜在的锁（默认为RLock）释放
        ## 然后等待notify()或者notify_all释放self._waiters中的锁后，新锁导致的堵塞状态解除，之后再次去获取潜在的锁，获取后真正恢复执行
        condition.wait()
        print("after", n)
    print("end {}".format(n))
    with condition:
        ## notify(n=1) n为唤醒的线程数，notify_all()则唤醒全部，但各线程仍然互斥，只有拥有condition潜在锁的线程能够得到执行
        ## notify实质是释放self._waiters中的锁，使得被这些锁阻塞的等待线程得到释放
        condition.notify_all()

def condition_learn():
    condition = threading.Condition()
    t1 = threading.Thread(target=wait_condition, args=(condition, 1))
    t2 = threading.Thread(target=wait_condition, args=(condition, 2))
    t1.start()
    t2.start()

    with condition:
        print("main")
        condition.notify_all()
```

内部实现

```python
# 下面我们看一下Python内部是如何实现条件同步机制的。如果用户没有传入锁（lock）对象，condition类的构造器创建一个可重入锁（RLock），这个锁将会在调用acquire()和release()时使用。
class _Condition(_Verbose):

    def __init__(self, lock=None, verbose=None):
        _Verbose.__init__(self, verbose)
        if lock is None:
            lock = RLock()
        self.__lock = lock
        
# 接下来是wait()方法。为了简化说明，我们假定在调用wait()方法时不使用timeout参数。wait()方法创建了一个名为waiter的锁，并且设置锁的状态为locked,然后将该新锁存入self.waiters中，释放condition锁，再次请求该锁，进入阻塞状态，等待notify将该锁释放以继续进行
def wait(self, timeout=None):
    ...
    # 创建新锁
    waiter = _allocate_lock()
    waiter.acquire()
    self.__waiters.append(waiter)
    # 释放condition 锁
    saved_state = self._release_save()
    try:    # 无论如何恢复状态 (例如, KeyboardInterrupt)
        if timeout is None:
            waiter.acquire()
            ...
        ...
    finally:
        # 重新获取condition锁
        self._acquire_restore(saved_state)
        
# 当生产者调用notify()方法时，notify()释放waiter锁，唤醒被阻塞的消费者。
def notify(self, n=1):
    ...
    __waiters = self.__waiters
    waiters = __waiters[:n]
    ...
    for waiter in waiters:
        waiter.release()
        try:
            __waiters.remove(waiter)
        except ValueError:
            pass
```

### Condition VS Event
- 根据以上分析，可知对于Condition，无论是否notify_all()还是notify(n)，同一时刻仅有一个线程能够拿到潜在的锁，故当多个线程需要访问共享资源，同时存在修改操作，需要加锁时，使用condition；而对于event来说，并不包含锁，故当set时，一次性唤醒所有等待线程

### Queue
使用queue（线程安全）来实现线程间数据通信，共享

#### 生产者慢于消费者时，会导致线程提前结束
```python
def queue_producer(q):
    for i in range(3):
        time.sleep(1)
        q.put(1)

def queue_consumer(q):
    for i in range(10):
        ## 应判断队列是否为空，但是这种情况在生产速度慢于消费速度的情况下是会出问题的,应当采取在生产结束时，在队列中
        ## put一个特殊对象来判断队列是否结束
        if q.empty():
            break
        print(q.get())
        q.task_done()

def queue_sync():
    q = Queue()
    t1 = threading.Thread(target=queue_producer, args=(q,))
    t2 = threading.Thread(target=queue_consumer, args=(q,))
    t1.start()
    t2.start()
    ## 在生产速度慢于消费速度的情况，会出现q.join()提前结束，原因在于queue判断线程全部结束的理由为queue队列为空，同时queue
    ## 维护着一个计数，当get成功时，该计数加一，当执行一个task_done()时该计数减一，该计数也为0时，认为线程全部结束，
    ## 这样在生产较慢的情形下，会提前满足条件而停止阻塞
    q.join()
    print("done")
```

可以使用结束哨兵来判断生产是否结束，生产者结束时向队列中加入object()，消费者检测结束状态